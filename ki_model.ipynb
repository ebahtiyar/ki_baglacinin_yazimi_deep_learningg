{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ki_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNH3Ku/Om1yrcZmPVJuqXVe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebahtiyar/ki_baglacinin_yazimi_deep_learningg/blob/main/ki_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdyhJyz7VozS"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Activation , Input,RepeatVector, TimeDistributed,Bidirectional\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/My Drive/project\")\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random as r\n",
        "import punc_preprocessing as punc\n",
        "import pickle\n",
        "import warnings\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import functions as f\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwvTGgeGVwja",
        "outputId": "dfd092f1-f976-4997-e3c1-899dc938c844"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions**"
      ],
      "metadata": {
        "id": "VBkXGSb9P_29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_str_index(text,index=0,replacement=''):\n",
        "    return '%s%s%s'%(text[:index],replacement,text[index+1:])\n",
        "\n",
        "def find_de_da_ki():\n",
        "    data = punc.load_data(\"/content/drive/My Drive/project/database.db\",\"Total_senteces_F1\")\n",
        "    c_data = []\n",
        "    for line in data:\n",
        "        words = line.split()\n",
        "        if len(words) < 35:\n",
        "          c_data.append(line)\n",
        "    da_de = []\n",
        "    ki = []\n",
        "    for i in c_data:\n",
        "        i = i.replace(\"’\",\"\")\n",
        "        i = re.sub(r'[^\\w\\s]', '', i)\n",
        "        i = i.replace(\"madem ki\",\"mademki\")\n",
        "        i = i.replace(\"meğer ki\",\"meğerki\")\n",
        "        i = i.replace(\"oysa ki\",\"oysaki\")\n",
        "        try:\n",
        "            if i.find(\"da\") != -1 :\n",
        "                index  = i.find(\"da\")\n",
        "                if i[index + 2] == \" \" and i[index-1] != \"’\":\n",
        "                    da_de.append(i)\n",
        "            if i.find(\"de\") != -1:\n",
        "                index = i.find(\"de\")\n",
        "                if i[index + 2] == \" \" and i[index-1] != \"’\":\n",
        "                    da_de.append(i)\n",
        "            if i.find(\"ki\") != -1:\n",
        "                index = i.find(\"ki\")\n",
        "                if i[index + 2] == \" \" and i.find(\"iki\") == -1 and i.find(\"eski\") == -1:\n",
        "                    ki.append(i)\n",
        "                                      \n",
        "        except:\n",
        "            pass\n",
        "    return da_de,ki"
      ],
      "metadata": {
        "id": "woIjOE9aVz87"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_simplify_conjunction():\n",
        "    da_de,ki = find_de_da_ki()\n",
        "    \n",
        "    sp_da_de = []\n",
        "    sp_ki = []\n",
        "    \n",
        "    adj_da_de = []\n",
        "    adj_ki = []\n",
        "    \n",
        "    \n",
        "    for i in da_de:\n",
        "        try:\n",
        "            if i.find(\"da\") != -1:\n",
        "                index = i.find(\"da\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_da_de.append(i)\n",
        "                else:\n",
        "                    adj_da_de.append(i)\n",
        "            if i.find(\"de\") != -1:\n",
        "                index = i.find(\"de\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_da_de.append(i)\n",
        "                else:\n",
        "                    adj_da_de.append(i)           \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    for i in ki:\n",
        "        try:\n",
        "            if i.find(\"ki\") != -1:\n",
        "                index = i.find(\"ki\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_ki.append(i)\n",
        "                else:\n",
        "                    adj_ki.append(i)         \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return sp_da_de,sp_ki,adj_da_de,adj_ki  "
      ],
      "metadata": {
        "id": "wpOXk-puWHqS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokenizer(line):\n",
        "        line = re.sub(r'[^\\w\\s]', '', line)\n",
        "        words = line.split()\n",
        "        r_words = []\n",
        "        try:\n",
        "            for pos,word in enumerate(words):\n",
        "                n = \"\"\n",
        "                if pos > 0:\n",
        "                   if word == \"ki\":\n",
        "                      before_word = words[pos-1]\n",
        "                      n = before_word + \" ki\"\n",
        "                      r_words.remove(before_word)\n",
        "                      r_words.append(n)\n",
        "                   else:\n",
        "                       r_words.append(word)\n",
        "                else:\n",
        "                    r_words.append(word)\n",
        "        except:\n",
        "            pass\n",
        "                \n",
        "        return r_words"
      ],
      "metadata": {
        "id": "sVClm_pgXQ96"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createTokenizer(data,Idx):\n",
        "  output = []\n",
        "  for line in data:\n",
        "      wordIndices = []\n",
        "      words = word_tokenizer(line)\n",
        "      for word in words:\n",
        "        if word in Idx:\n",
        "          wordIdx = Idx[word].index\n",
        "        elif word.lower() in Idx:\n",
        "          wordIdx = Idx[word.lower()].index\n",
        "        else:\n",
        "          wordIdx = Idx['UNK'].index\n",
        "        wordIndices.append(wordIdx)\n",
        "      output.append(wordIndices)\n",
        "  return output"
      ],
      "metadata": {
        "id": "PbFYBilUXbgK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createTokenizer1(data,Idx):\n",
        "  output = []\n",
        "  for line in data:\n",
        "      wordIndices = []\n",
        "      words = word_tokenizer(line)\n",
        "      for word in words:\n",
        "        if word in Idx:\n",
        "          wordIdx = Idx[word]\n",
        "        elif word.lower() in Idx:\n",
        "          wordIdx = Idx[word.lower()]\n",
        "        else:\n",
        "          wordIdx = Idx['UNK']\n",
        "        wordIndices.append(wordIdx)\n",
        "      output.append(wordIndices)\n",
        "  return output"
      ],
      "metadata": {
        "id": "PXWgutXZXduy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_training_output(line):\n",
        "    \n",
        "        label = \"\"\n",
        "        r_words = word_tokenizer(line)\n",
        "                    \n",
        "        for word in r_words:\n",
        "            if word.find(\"ki\") > 0:\n",
        "                if len(word) > 1:\n",
        "                    if word[len(word) - 2] == \"k\":\n",
        "                        label = label + \"E \"\n",
        "                    else:\n",
        "                         label = label + \"O \"\n",
        "                else:\n",
        "                    label = label + \"O \"\n",
        "            else:\n",
        "                label = label + \"O \"\n",
        "        label = label.strip()\n",
        "        \n",
        "          \n",
        "        return label"
      ],
      "metadata": {
        "id": "MrHGOtJHXGYK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_training_input(data):\n",
        "    training_input = []\n",
        "    training_label = []\n",
        "    for line in data:\n",
        "        line = line.strip()\n",
        "        training_input.append(line)\n",
        "        r_words = word_tokenizer(line)\n",
        "        label = \"\"\n",
        "        for word in r_words:\n",
        "            label = label + \"O \"\n",
        "        training_label.append(label)\n",
        "        \n",
        "        b_da = [m.start() for m in re.finditer('ki ', line)]\n",
        "        k  = 0\n",
        "        for pos in b_da:\n",
        "            if pos > 0:\n",
        "                pos = pos + k\n",
        "                if line[pos-1] == \" \":\n",
        "                    line = replace_str_index(line,index=pos-1,replacement='')\n",
        "                    k = k - 1\n",
        "                        \n",
        "                elif line[pos-1].isalpha():\n",
        "                    line = replace_str_index(line,index=pos,replacement=' k')\n",
        "                    k = k + 1\n",
        "                    \n",
        "        training_input.append(line)\n",
        "        label1 = generator_training_output(line)\n",
        "        training_label.append(label1)\n",
        "        \n",
        "    return training_input,training_label"
      ],
      "metadata": {
        "id": "yNpU9sb6W8fs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preliminary**"
      ],
      "metadata": {
        "id": "ZwwQEuuBQHsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp_da_de,sp_ki,adj_da_de,adj_ki = data_simplify_conjunction()\n",
        "data = adj_ki + sp_ki\n",
        "ki_train , ki_test = train_test_split(data,test_size=0.1)\n",
        "input_data = ki_train\n",
        "input_data = f.unique(input_data)\n",
        "training_input,training_label = generator_training_input(input_data)\n",
        "\n",
        "s = list(zip(training_input,training_label))\n",
        "r.shuffle(s)\n",
        "training_input,training_label = zip(*s)\n"
      ],
      "metadata": {
        "id": "VFRO7XhmXFii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Separate Ki:\" + str(len(sp_ki)))\n",
        "print(\"Total Adjacent  Ki:\" + str(len(adj_ki)))\n",
        "print(\"Total Trainig Sentences: \" + str(len(training_input)))\n",
        "print(\"Total Testing Sentences: \" + str(len(ki_test)))\n",
        "print(\"İnput Sentence:\",training_input[4321])\n",
        "print(\"Label Sentence:\",training_label[4321])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ZCSUBBPhet",
        "outputId": "c6e66030-49cf-472b-a492-7923976bb445"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Separate Ki:62710\n",
            "Total Adjacent  Ki:213981\n",
            "Total Trainig Sentences: 416614\n",
            "Total Testing Sentences: 27670\n",
            "İnput Sentence: Toksöz ailesinin sahipliğinde ki Sağra çalışanlarının sendikası Genel İşten şikayetler aldık\n",
            "Label Sentence: O O E O O O O O O O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre Training using Word2Vec Model**"
      ],
      "metadata": {
        "id": "ldICB9_nPuWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traning_token =[]\n",
        "for i in training_input:\n",
        "  i = i.lower()\n",
        "  traning_token.append(word_tokenizer(i))\n",
        "traning_token.append([\"UNK\",\"PAD\"])"
      ],
      "metadata": {
        "id": "idejbNZhdZwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Word2Vec_model = Word2Vec(min_count=1)\n",
        "Word2Vec_model.build_vocab(traning_token)  # prepare the model vocabulary\n",
        "Word2Vec_model.train(traning_token, total_examples=Word2Vec_model.corpus_count, epochs= 100)  # train word vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmvepb2oeM2i",
        "outputId": "6d3aabf3-9e56-4a64-c1be-6a5594e65c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(493322283, 531742000)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Word2Vec_model.save(\"/content/drive/MyDrive/project/savedd_model/Word2Vec_model_ki\")"
      ],
      "metadata": {
        "id": "m06_jrLSeOup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Word2Vec_model=Word2Vec.load(\"/content/drive/MyDrive/project/savedd_model/Word2Vec_model_ki\")\n",
        "pretrained_weights = Word2Vec_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape"
      ],
      "metadata": {
        "id": "4qbOLCXKeR_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c7bfb9-f3eb-4707-99de-126af9ce7143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokinezer and Padding Processing**"
      ],
      "metadata": {
        "id": "QEzo7JvLP1HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tokenizer = createTokenizer(training_input,Word2Vec_model.wv.vocab)\n",
        "\n",
        "labelSet = set()\n",
        "for line in training_label:\n",
        "  words = word_tokenizer(line)\n",
        "  for word in words:\n",
        "    labelSet.add(word.lower())\n",
        "\n",
        "sorted_labels = sorted(list(labelSet), key=len)\n",
        "# Create mapping for labels\n",
        "label2Idx = {}\n",
        "if len(label2Idx) == 0:\n",
        "  label2Idx[\"PAD\"] = len(label2Idx)\n",
        "  label2Idx[\"UNK\"] = len(label2Idx)\n",
        "for label in sorted_labels:\n",
        "  label2Idx[label] = len(label2Idx)\n",
        "idx2Label = {v: k for k, v in label2Idx.items()}\n",
        "\n",
        "y_train_tokenizer = createTokenizer1(training_label,label2Idx)"
      ],
      "metadata": {
        "id": "NVR-QPdMebN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_in_len = 34\n",
        "max_out_len = 34\n",
        "x_train_pad = pad_sequences(x_train_tokenizer, max_in_len, padding = \"post\")\n",
        "y_train_pad = pad_sequences(y_train_tokenizer, max_out_len, padding = \"post\")\n",
        "\n",
        "x_train_pad = x_train_pad.reshape(*x_train_pad.shape, 1)\n",
        "y_train_pad = y_train_pad.reshape(*y_train_pad.shape, 1)\n",
        "print(\"Max Length of Train Sentences:\" + str(max_in_len) + \" Max Length of Train Labels:\" + str(max_out_len))"
      ],
      "metadata": {
        "id": "tR0Q-10EerZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bidirectional Model**"
      ],
      "metadata": {
        "id": "CVlQwXOhPYP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "callbacks = [checkpoint]"
      ],
      "metadata": {
        "id": "oDJ8Mevcetgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main = Sequential()\n",
        "model_main.add(Input(shape=(max_in_len,)))\n",
        "model_main.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size,weights=[pretrained_weights]))\n",
        "model_main.add(Bidirectional(LSTM(units=emdedding_size,return_sequences=False)))\n",
        "model_main.add(RepeatVector(max_out_len))\n",
        "model_main.add(Bidirectional(LSTM(emdedding_size, return_sequences=True, dropout=0.1)))\n",
        "model_main.add(TimeDistributed(Dense(len(label2Idx)+1)))\n",
        "model_main.add(Activation('softmax'))\n",
        "model_main.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5G1BIfvfe4nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main.fit(x_train_pad, y_train_pad, validation_split = 0.2, epochs= 10 , callbacks=callbacks, batch_size = 256)"
      ],
      "metadata": {
        "id": "OZrdFrkMe6Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main.save('/content/drive/My Drive/project/savedd_model/model_ki_w2v_1.h5')"
      ],
      "metadata": {
        "id": "AkZogGmbe9aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testting**"
      ],
      "metadata": {
        "id": "SzaDhhhSnLy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/drive/My Drive/project/savedd_model/model_ki_w2v_1.h5')"
      ],
      "metadata": {
        "id": "qxx-zXU7fCRS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2Label = {0 :'PAD', 1:'UNK' , 2: 'o' , 3: 'e'}\n",
        "Word2Vec_model=Word2Vec.load(\"/content/drive/MyDrive/project/savedd_model/Word2Vec_model_ki\")"
      ],
      "metadata": {
        "id": "jpcJerHLfJfw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model_main,sample,Word2Vec_model,idx2Label):\n",
        "  sample_pad = []\n",
        "  words = word_tokenizer(sample.lower())\n",
        "  for word in words:\n",
        "      if word in Word2Vec_model.wv.vocab:\n",
        "          \n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[word].index)\n",
        "      else:\n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[\"UNK\"].index)\n",
        "\n",
        "  sample_pad = pad_sequences([sample_pad], maxlen=34, padding='post')\n",
        "\n",
        "  pred = model_main.predict(sample_pad)\n",
        "  label = [idx2Label[np.argmax(x)] for x in pred[0]]\n",
        "  output_label = \"\"\n",
        "  for i in label:\n",
        "      output_label = output_label + i + \" \"\n",
        "\n",
        "  pred_sent = \"\"\n",
        "  for i in zip(words,label):\n",
        "    if i[1] == \"o\":\n",
        "      pred_sent = pred_sent +  i[0] + \" \"\n",
        "    elif i[1] == \"e\":\n",
        "        if i[0].find(\" ki\") > 0:\n",
        "            new_word = replace_str_index(i[0],index=i[0].find(\" ki\"),replacement='')\n",
        "            pred_sent = pred_sent + new_word + \" \"\n",
        "        elif i[0].find(\" \") == -1 and i[0].find(\"ki\") == len(i[0]) - 2:\n",
        "            new_word = replace_str_index(i[0],i[0].find(\"ki\"),replacement=' k')\n",
        "            pred_sent = pred_sent + new_word + \" \"\n",
        "        else:\n",
        "            pred_sent = pred_sent + i[0] + \" \"\n",
        "            \n",
        "    elif i[0][1] == \"PAD\":\n",
        "        continue\n",
        "  pred_sent = pred_sent.strip()\n",
        "\n",
        "  r_words = word_tokenizer(pred_sent)\n",
        "  return pred_sent"
      ],
      "metadata": {
        "id": "UFlxf3TefJmQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_testing_input(data):\n",
        "    training_input = []\n",
        "    training_label = []\n",
        "    for line in data:\n",
        "        line = line.strip()\n",
        "        b_da = [m.start() for m in re.finditer('ki ', line)]\n",
        "        k  = 0\n",
        "        for pos in b_da:\n",
        "            if pos > 0:\n",
        "                pos = pos + k\n",
        "                if line[pos-1] == \" \":\n",
        "                    line = replace_str_index(line,index=pos-1,replacement='')\n",
        "                    k = k - 1\n",
        "                        \n",
        "                elif line[pos-1].isalpha():\n",
        "                    line = replace_str_index(line,index=pos,replacement=' k')\n",
        "                    k = k + 1\n",
        "                    \n",
        "        training_input.append(line)\n",
        "    \n",
        "    return training_input"
      ],
      "metadata": {
        "id": "TYwmpdD_fJrU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_ki_test = punc.n_punc(ki_test,\" ki \")\n",
        "\n",
        "test = r.choices(sp_ki_test,k=1000) + r.choices(adj_ki,k = 1000)\n",
        "testing_input = generator_testing_input(test)"
      ],
      "metadata": {
        "id": "ZXFR2KjnGuWo"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "pred_last = []\n",
        "wrong = []\n",
        "wrong_pos = []\n",
        "for line in testing_input:\n",
        "    pred.append(prediction(model,line.lower(),Word2Vec_model,idx2Label).strip())\n",
        "   \n",
        "k = 0\n",
        "for pos,line in enumerate(test):\n",
        "    line = line.lower()\n",
        "    line = line.replace(\"i̇\",\"i\")\n",
        "    if line.lower() == pred[pos]:\n",
        "        k = k + 1\n",
        "    else:\n",
        "      wrong.append(pred[pos])\n",
        "      wrong_pos.append(pos)"
      ],
      "metadata": {
        "id": "8E8lDC2sDalI"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand = r.randrange(100)\n",
        "print(\"Score: \" + str(k) +\" in \" + str(len(testing_input)) + \" sentences\")\n",
        "print(\"Orginal: \"+test[rand].lower())\n",
        "print(\"Wrong: \"+testing_input[rand].lower())\n",
        "print(\"Prediction: \"+pred[rand])"
      ],
      "metadata": {
        "id": "lVowtGNOFq5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e36cdc-c439-4a86-b7f7-413ec221fc7b"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 1855 in 2000 sentences\n",
            "Orginal: herkesin kendi içerisinde büyük planları var tabii ki isterim\n",
            "Wrong: herkesin kendi içerisinde büyük planları var tabiiki isterim\n",
            "Prediction: herkesin kendi içerisinde büyük planları var tabii ki isterim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CALCULATE SCORE**"
      ],
      "metadata": {
        "id": "7WLF3OXNK-VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_score(model_main,sample,Word2Vec_model,idx2Label):\n",
        "  sample_pad = []\n",
        "  words = word_tokenizer(sample.lower())\n",
        "  for word in words:\n",
        "      if word in Word2Vec_model.wv.vocab:\n",
        "          \n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[word].index)\n",
        "      else:\n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[\"UNK\"].index)\n",
        "\n",
        "  sample_pad = pad_sequences([sample_pad], maxlen=34, padding='post')\n",
        "\n",
        "  pred = model_main.predict(sample_pad)\n",
        "  label = [idx2Label[np.argmax(x)] for x in pred[0]]\n",
        "\n",
        "  return label"
      ],
      "metadata": {
        "id": "jpPMpRY_tMmU"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label = []\n",
        "\n",
        "for line in testing_input:\n",
        "    pred_label.append(prediction_score(model,line.lower(),Word2Vec_model,idx2Label))"
      ],
      "metadata": {
        "id": "zSvFeVlmtcgF"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label_l = []\n",
        "\n",
        "for pred in pred_label:\n",
        "    t = []\n",
        "    for i in pred:\n",
        "        if i != \"PAD\":\n",
        "           if i == \"e\":\n",
        "              i = \"DEC\"\n",
        "           elif i == \"o\":\n",
        "              i = \"CON\"\n",
        "           t.append(i.upper())\n",
        "        else:\n",
        "          break\n",
        "    pred_label_l.append(t)"
      ],
      "metadata": {
        "id": "8lAk5pQEvtdH"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = []\n",
        "\n",
        "for line in testing_input:\n",
        "    test_label.append(generator_training_output(line).lower())\n",
        "test = []\n",
        "for line in test_label:\n",
        "     line = line.upper()\n",
        "     x =  []\n",
        "     for l in line.split():\n",
        "         if l == \"O\":\n",
        "            i = \"CON\"\n",
        "         else:\n",
        "            i = \"DEC\"\n",
        "         x.append(i)\n",
        "     test.append(x)\n",
        "\n",
        "test_x = []\n",
        "text_label = []\n",
        "for i in range(len(test)):\n",
        "  if len(test[i]) == len(pred_label_l[i]):\n",
        "      test_x.append(test[i])\n",
        "      text_label.append(pred_label_l[i]) "
      ],
      "metadata": {
        "id": "Wp31DFWMyGsL"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = classification_report(test_x,text_label)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFlFPx4cHKl_",
        "outputId": "bcf80b9f-f82e-4692-c6de-676dd9087acc"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CON seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DEC seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          EC       0.98      0.93      0.96      2168\n",
            "          ON       0.95      0.93      0.94      3588\n",
            "\n",
            "   micro avg       0.96      0.93      0.95      5756\n",
            "   macro avg       0.97      0.93      0.95      5756\n",
            "weighted avg       0.97      0.93      0.95      5756\n",
            "\n"
          ]
        }
      ]
    }
  ]
}